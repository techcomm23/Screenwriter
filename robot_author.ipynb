{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "The goal of this project is to build an RNN model that can generate text. We will utilize real movie script to train this model. Later, we will use the trained model to author/generate scripts for other movies from scratch.\n",
    "\n",
    "## Import Libraries and Load Data\n",
    "\n",
    "The first step is to import relevant libraries required for this project. The raw movie script is saved in `./data/MovieScript.txt`. We also need to load this data as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# load data\n",
    "data_dir = './data/MovieScript.txt'  \n",
    "text = helper.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data\n",
    "To understand our data better, we need extract some statistics. Next we can use the variable `explore_range` to explore specific lines within our script. We will find that the raw text has been preprocessed as follows:\n",
    "- All letters are small letter\n",
    "- each new line of the raw text is separated by newline character `\\n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Statistics:========\n",
      "Number of unique words: 46367\n",
      "Number of lines: 109233\n",
      "Average number of words in each line: 5.544240293684143\n",
      "========================\n",
      "\n",
      "The lines 33 to 43:\n",
      "\n",
      "george: no, you didnt! \n",
      "\n",
      "jerry: i thought i told you about it, yes, she teaches political science? i met her the night i did the show in lansing... \n",
      "\n",
      "george: ha. \n",
      "\n",
      "jerry: (looks in the creamer) theres no milk in here, what... \n",
      "\n",
      "george: wait wait wait, what is she... (takes the milk can from jerry and puts it on the table) what is she like? \n"
     ]
    }
   ],
   "source": [
    "explore_range = (33, 43)\n",
    "\n",
    "print('Data Statistics:========')\n",
    "print('Number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "print('========================')\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*explore_range))\n",
    "print('\\n'.join(text.split('\\n')[explore_range[0]:explore_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Preprocessing\n",
    "In order to prepare our raw data to be used as input to our model, we need to preprocess it. We will develop two functions that will be used to achieve this goal:\n",
    "- Embedding lookup\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Embedding Lookup\n",
    "Used to build word embedding by converting words into tokens and vice versa. This function will accept text as input and will return tuple dictionaries: \n",
    "- `vocab_to_int`: a dictionary that transforms words to integer \n",
    "- `int_to_vocab`: a dictionary that transforms integer to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def create_lookup_tables(text):\n",
    "\n",
    "    from collections import Counter\n",
    "    # text --> {text:count}\n",
    "    word_counts = Counter(text)\n",
    "    # sorting the words from most to least frequent in text occurrence\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    # create dictionaries\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "    \n",
    "    # return tuple\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "\n",
    "# test function\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks can create multiple ids for the same word. For example, \"bye\" and \"bye!\" would generate two different word ids.\n",
    "\n",
    "We will implement the function `token_lookup` to return a dictionary that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  This function will act as lookup table for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( **.** )\n",
    "- Comma ( **,** )\n",
    "- Quotation Mark ( **\"** )\n",
    "- Semicolon ( **;** )\n",
    "- Exclamation mark ( **!** )\n",
    "- Question mark ( **?** )\n",
    "- Left Parentheses ( **(** )\n",
    "- Right Parentheses ( **)** )\n",
    "- Dash ( **-** )\n",
    "- Return ( **\\n** )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    punct = {\n",
    "        '.':'||PERIOD||',\n",
    "        ',':'||COMMA||',\n",
    "        '\"':'||QUOTATION_MARK||',\n",
    "        ';':'||SEMICOLON||',\n",
    "        '!':'||EXCLAMATION_MARK||',\n",
    "        '?':'||QUESTION_MARK||',\n",
    "        '(':'||LEFT_PAREN||',\n",
    "        ')':'||RIGHT_PAREN||',\n",
    "        '?':'||QUESTION_MARK||',\n",
    "        '\\n':'||NEW_LINE||',\n",
    "        '-':'||DASH||'\n",
    "    }\n",
    "\n",
    "    return punct\n",
    "\n",
    "# test function\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process all the data and save it\n",
    "\n",
    "The function `preprocess_and_save_data` will apply above lookups to preprocess our raw data. Running the code cell below will pre-process all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process training data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "the code bellow will create a checkpoint and is used to load saved data and reapply them to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create checkpoint\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "To build our RNN model, we need to implement the following components:\n",
    "* RNN Module \n",
    "* Forward function\n",
    "* Backpropagation function\n",
    "\n",
    "### Check Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:117: UserWarning: \n",
      "    Found GPU1 Quadro K4000 which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# detect cuda card\n",
    "torch.cuda.current_device()\n",
    "#torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching Input Data\n",
    "Before we build our model we need to batch our input data. To achieve this, we will build the function `batch_data` to do the following:\n",
    "1. Break data: The function `batch_data` breaks up word id's into the appropriate sequence lengths, such that only complete sequence lengths are constructed.\n",
    "2. Create data: weâ€™ll use [TensorDataset](http://pytorch.org/docs/master/data.html#torch.utils.data.TensorDataset) to convert data into Tensors and formatted with TensorDataset. \n",
    "3. Batch data: use [DataLoader](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader), to handle batching, shuffling, and other dataset iteration functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \n",
    "    # calculate total number of batches\n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # Clip extra characters to create only full batches\n",
    "    words = words[:n_batches * batch_size]\n",
    "    # initiate features, target lists\n",
    "    x = [] \n",
    "    y = []\n",
    "    # index of last sequence:\n",
    "    last_seq = len(words) - sequence_length\n",
    "    # iterate through the words up to the beginning of last seq in \"words\":\n",
    "    for n in range(0, last_seq):\n",
    "        # The features\n",
    "        x.append(words[n:n+sequence_length])\n",
    "        # The targets, shifted by one\n",
    "        y.append(words[n+sequence_length])\n",
    "    feature_tensors = torch.from_numpy(np.asarray(x))\n",
    "    target_tensors = torch.from_numpy(np.asarray(y))\n",
    "\n",
    "    # create and batch data\n",
    "    data = TensorDataset(feature_tensors, target_tensors)\n",
    "    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test batch function\n",
    "To test our function we will generate a sample data `test_text` and we will observe if batching both the input sequence (sample_x) and the target (sample_y) is correct.\n",
    "\n",
    "#### Sizes: \n",
    "sample_x should be of size (batch_size, sequence_length) and sample_y should just have one dimension: batch_size.\n",
    "#### Values: \n",
    "targets, sample_y, shoud be the next value in the ordered test_text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[ 7,  8,  9, 10, 11],\n",
      "        [14, 15, 16, 17, 18],\n",
      "        [34, 35, 36, 37, 38],\n",
      "        [18, 19, 20, 21, 22],\n",
      "        [15, 16, 17, 18, 19],\n",
      "        [33, 34, 35, 36, 37],\n",
      "        [19, 20, 21, 22, 23],\n",
      "        [21, 22, 23, 24, 25],\n",
      "        [27, 28, 29, 30, 31],\n",
      "        [44, 45, 46, 47, 48]], dtype=torch.int32)\n",
      "\n",
      "torch.Size([10])\n",
      "tensor([12, 19, 39, 23, 20, 38, 24, 26, 32, 49], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "\n",
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build the Neural Network\n",
    "\n",
    "### RNN Module\n",
    "We can implement our RNN using any PyTorch's [Module class](http://pytorch.org/docs/master/nn.html#torch.nn.Module). In this case we will use LSTM.\n",
    "\n",
    "### Class functions\n",
    "We will implement the following functions for the class:\n",
    " - `__init__` - The initialize function. \n",
    " - `init_hidden` - The initialization function for LSTM hidden state\n",
    " - `forward` - Forward propagation function.\n",
    " \n",
    "The initialize function should create the layers of the neural network and save them to the class. The forward propagation function will use these layers to run forward propagation and generate an output and a hidden state.\n",
    "\n",
    "### Model output\n",
    "**The output of this model should be the *last* batch of word scores** after a complete sequence has been processed. That is, for each input sequence of words, we only want to output the word scores for a single, most likely, next word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        ## set class variables\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        ## define model layers\n",
    "        # define the embedding lookup table\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # define the LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "\n",
    "        # define batch size\n",
    "        batch_size = nn_input.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        nn_input = nn_input.long() \n",
    "        embeds = self.embedding(nn_input)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs and copy tensor\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # fully-connected layer\n",
    "        output = self.fc(lstm_out)\n",
    "\n",
    "        # reshape into (batch_size, seq_length, output_size)\n",
    "        output = output.view(batch_size, -1, self.output_size)\n",
    "        \n",
    "        # get last batch\n",
    "        out = output[:, -1]\n",
    "        \n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "# test function\n",
    "tests.test_rnn(RNN, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define forward and backpropagation\n",
    "\n",
    "Apply forward and back propagation using our RNN class. This function will be called, iteratively, in the training loop as follows:\n",
    "```\n",
    "loss = forward_back_prop(decoder, decoder_optimizer, criterion, inp, target)\n",
    "```\n",
    "\n",
    "This function will return the average loss over a batch and the hidden state returned by a call to `RNN(inp, hidden)`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param rnn: The PyTorch Module that holds the neural network\n",
    "    :param optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # move data to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        rnn.cuda()\n",
    "        inp, target = inp.cuda(), target.cuda()\n",
    "\n",
    "    h = tuple([each.data for each in hidden])\n",
    "\n",
    "    # zero accumulated gradients\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # get the output from the model\n",
    "    output, h = rnn(inp, h)\n",
    "\n",
    "    # calculate the loss and perform backprop \n",
    "    loss = criterion(output, target.long()) \n",
    "    loss.backward()\n",
    "\n",
    "    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 5) #clip = 5\n",
    "    optimizer.step()\n",
    "\n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), h\n",
    "\n",
    "\n",
    "# test (only general checks on the expected outputs of our functions)\n",
    "tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "\n",
    "With the structure of the network complete and data ready to be fed in the neural network, it's time to train it.\n",
    "\n",
    "### Train Loop\n",
    "\n",
    "`train_rnn` function will train the network over all the batches for the number of epochs given. The model progress will be shown every number of batches. This number is set with the `show_every_n_batches` parameter. We'll set this parameter along with other parameters in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Set and train the neural network with the following parameters:\n",
    "- Set `sequence_length` to the length of a sequence.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `num_epochs` to the number of epochs to train for.\n",
    "- Set `learning_rate` to the learning rate for an Adam optimizer.\n",
    "- Set `vocab_size` to the number of unique tokens in our vocabulary.\n",
    "- Set `output_size` to the desired size of the output.\n",
    "- Set `embedding_dim` to the embedding dimension; smaller than the vocab_size.\n",
    "- Set `hidden_dim` to the hidden dimension of your RNN.\n",
    "- Set `n_layers` to the number of layers/cells in your RNN.\n",
    "- Set `show_every_n_batches` to the number of batches at which the neural network should print progress.\n",
    "\n",
    "If the network isn't getting the desired results, we can tweak these parameters and/or the layers in the `RNN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data params\n",
    "# Sequence Length\n",
    "sequence_length = 10   # of words in a sequence\n",
    "\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "\n",
    "# data loader - do not change\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 20 \n",
    "\n",
    "# Learning Rate\n",
    "learning_rate = 0.001 \n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(vocab_to_int)\n",
    "# Output size\n",
    "output_size = vocab_size\n",
    "# Embedding Dimension\n",
    "embedding_dim = 400 \n",
    "\n",
    "# Hidden Dimension\n",
    "hidden_dim = 512 \n",
    "\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "In the next cell, we will train the neural network on the pre-processed data.  We will experiment with different hyperparameters until we get acceptable loss (3.5 in this case). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20 epoch(s)...\n",
      "Epoch:    1/20    Loss: 5.267440577507019\n",
      "\n",
      "Epoch:    1/20    Loss: 4.636298000335693\n",
      "\n",
      "Epoch:    1/20    Loss: 4.448770101070404\n",
      "\n",
      "Epoch:    1/20    Loss: 4.325790477275849\n",
      "\n",
      "Epoch:    1/20    Loss: 4.25684912776947\n",
      "\n",
      "Epoch:    1/20    Loss: 4.228826926708221\n",
      "\n",
      "Epoch:    1/20    Loss: 4.201540566921234\n",
      "\n",
      "Epoch:    1/20    Loss: 4.138343175411224\n",
      "\n",
      "Epoch:    1/20    Loss: 4.125659349441528\n",
      "\n",
      "Epoch:    1/20    Loss: 4.0906209759712215\n",
      "\n",
      "Epoch:    1/20    Loss: 4.060240993499756\n",
      "\n",
      "Epoch:    1/20    Loss: 4.041012619018555\n",
      "\n",
      "Epoch:    1/20    Loss: 4.042131265163421\n",
      "\n",
      "Epoch:    2/20    Loss: 3.902938900161381\n",
      "\n",
      "Epoch:    2/20    Loss: 3.8041995692253114\n",
      "\n",
      "Epoch:    2/20    Loss: 3.821500693321228\n",
      "\n",
      "Epoch:    2/20    Loss: 3.79368452501297\n",
      "\n",
      "Epoch:    2/20    Loss: 3.7887437047958374\n",
      "\n",
      "Epoch:    2/20    Loss: 3.798504003047943\n",
      "\n",
      "Epoch:    2/20    Loss: 3.7891750044822694\n",
      "\n",
      "Epoch:    2/20    Loss: 3.8044117736816405\n",
      "\n",
      "Epoch:    2/20    Loss: 3.776681932449341\n",
      "\n",
      "Epoch:    2/20    Loss: 3.793649692058563\n",
      "\n",
      "Epoch:    2/20    Loss: 3.771964218139648\n",
      "\n",
      "Epoch:    2/20    Loss: 3.778884696006775\n",
      "\n",
      "Epoch:    2/20    Loss: 3.7977480211257935\n",
      "\n",
      "Epoch:    3/20    Loss: 3.660293561121649\n",
      "\n",
      "Epoch:    3/20    Loss: 3.571159200191498\n",
      "\n",
      "Epoch:    3/20    Loss: 3.578093707561493\n",
      "\n",
      "Epoch:    3/20    Loss: 3.5850097184181213\n",
      "\n",
      "Epoch:    3/20    Loss: 3.604779048919678\n",
      "\n",
      "Epoch:    3/20    Loss: 3.5912372126579286\n",
      "\n",
      "Epoch:    3/20    Loss: 3.6077563428878783\n",
      "\n",
      "Epoch:    3/20    Loss: 3.5870429549217224\n",
      "\n",
      "Epoch:    3/20    Loss: 3.600592128753662\n",
      "\n",
      "Epoch:    3/20    Loss: 3.6046822834014893\n",
      "\n",
      "Epoch:    3/20    Loss: 3.608290143966675\n",
      "\n",
      "Epoch:    3/20    Loss: 3.619332379817963\n",
      "\n",
      "Epoch:    3/20    Loss: 3.627731348514557\n",
      "\n",
      "Epoch:    4/20    Loss: 3.501084210705166\n",
      "\n",
      "Epoch:    4/20    Loss: 3.3909828634262085\n",
      "\n",
      "Epoch:    4/20    Loss: 3.413759729385376\n",
      "\n",
      "Epoch:    4/20    Loss: 3.4273908767700196\n",
      "\n",
      "Epoch:    4/20    Loss: 3.4303259048461916\n",
      "\n",
      "Epoch:    4/20    Loss: 3.4630299291610718\n",
      "\n",
      "Epoch:    4/20    Loss: 3.4704899373054503\n",
      "\n",
      "Epoch:    4/20    Loss: 3.474830933570862\n",
      "\n",
      "Epoch:    4/20    Loss: 3.471839447975159\n",
      "\n",
      "Epoch:    4/20    Loss: 3.4690923800468445\n",
      "\n",
      "Epoch:    4/20    Loss: 3.4951754198074343\n",
      "\n",
      "Epoch:    4/20    Loss: 3.484898456096649\n",
      "\n",
      "Epoch:    4/20    Loss: 3.5165427870750428\n",
      "\n",
      "Epoch:    5/20    Loss: 3.3955512290651146\n",
      "\n",
      "Epoch:    5/20    Loss: 3.275792362689972\n",
      "\n",
      "Epoch:    5/20    Loss: 3.303857039451599\n",
      "\n",
      "Epoch:    5/20    Loss: 3.315778872013092\n",
      "\n",
      "Epoch:    5/20    Loss: 3.338555621623993\n",
      "\n",
      "Epoch:    5/20    Loss: 3.323338143348694\n",
      "\n",
      "Epoch:    5/20    Loss: 3.345915987491608\n",
      "\n",
      "Epoch:    5/20    Loss: 3.3645464758872987\n",
      "\n",
      "Epoch:    5/20    Loss: 3.363424832344055\n",
      "\n",
      "Epoch:    5/20    Loss: 3.381545832633972\n",
      "\n",
      "Epoch:    5/20    Loss: 3.381731057167053\n",
      "\n",
      "Epoch:    5/20    Loss: 3.4035324149131774\n",
      "\n",
      "Epoch:    5/20    Loss: 3.4176465101242064\n",
      "\n",
      "Epoch:    6/20    Loss: 3.2889061281996326\n",
      "\n",
      "Epoch:    6/20    Loss: 3.1817466487884523\n",
      "\n",
      "Epoch:    6/20    Loss: 3.1998275728225707\n",
      "\n",
      "Epoch:    6/20    Loss: 3.2480321483612062\n",
      "\n",
      "Epoch:    6/20    Loss: 3.2500911965370176\n",
      "\n",
      "Epoch:    6/20    Loss: 3.266485158443451\n",
      "\n",
      "Epoch:    6/20    Loss: 3.251429413318634\n",
      "\n",
      "Epoch:    6/20    Loss: 3.2709699912071226\n",
      "\n",
      "Epoch:    6/20    Loss: 3.2979928798675537\n",
      "\n",
      "Epoch:    6/20    Loss: 3.301584733009338\n",
      "\n",
      "Epoch:    6/20    Loss: 3.326108946800232\n",
      "\n",
      "Epoch:    6/20    Loss: 3.3166963238716125\n",
      "\n",
      "Epoch:    6/20    Loss: 3.31219430065155\n",
      "\n",
      "Epoch:    7/20    Loss: 3.212863906848529\n",
      "\n",
      "Epoch:    7/20    Loss: 3.1106288681030274\n",
      "\n",
      "Epoch:    7/20    Loss: 3.124540407180786\n",
      "\n",
      "Epoch:    7/20    Loss: 3.1678213319778443\n",
      "\n",
      "Epoch:    7/20    Loss: 3.1789676599502563\n",
      "\n",
      "Epoch:    7/20    Loss: 3.1701965336799622\n",
      "\n",
      "Epoch:    7/20    Loss: 3.1938233499526976\n",
      "\n",
      "Epoch:    7/20    Loss: 3.210370993614197\n",
      "\n",
      "Epoch:    7/20    Loss: 3.220309057235718\n",
      "\n",
      "Epoch:    7/20    Loss: 3.2424933552742004\n",
      "\n",
      "Epoch:    7/20    Loss: 3.2338655133247376\n",
      "\n",
      "Epoch:    7/20    Loss: 3.267603297710419\n",
      "\n",
      "Epoch:    7/20    Loss: 3.2831727643013\n",
      "\n",
      "Epoch:    8/20    Loss: 3.150821317818539\n",
      "\n",
      "Epoch:    8/20    Loss: 3.067401222705841\n",
      "\n",
      "Epoch:    8/20    Loss: 3.078298448085785\n",
      "\n",
      "Epoch:    8/20    Loss: 3.1004038996696472\n",
      "\n",
      "Epoch:    8/20    Loss: 3.0984106941223146\n",
      "\n",
      "Epoch:    8/20    Loss: 3.131437712192535\n",
      "\n",
      "Epoch:    8/20    Loss: 3.1432369685173036\n",
      "\n",
      "Epoch:    8/20    Loss: 3.1483602528572083\n",
      "\n",
      "Epoch:    8/20    Loss: 3.1542953996658327\n",
      "\n",
      "Epoch:    8/20    Loss: 3.1878628635406496\n",
      "\n",
      "Epoch:    8/20    Loss: 3.1906556701660156\n",
      "\n",
      "Epoch:    8/20    Loss: 3.2318593759536745\n",
      "\n",
      "Epoch:    8/20    Loss: 3.219580011367798\n",
      "\n",
      "Epoch:    9/20    Loss: 3.09709283611006\n",
      "\n",
      "Epoch:    9/20    Loss: 3.007457259654999\n",
      "\n",
      "Epoch:    9/20    Loss: 3.0362071061134337\n",
      "\n",
      "Epoch:    9/20    Loss: 3.0332302045822144\n",
      "\n",
      "Epoch:    9/20    Loss: 3.070320770740509\n",
      "\n",
      "Epoch:    9/20    Loss: 3.075437208652496\n",
      "\n",
      "Epoch:    9/20    Loss: 3.0905775184631348\n",
      "\n",
      "Epoch:    9/20    Loss: 3.0980382566452027\n",
      "\n",
      "Epoch:    9/20    Loss: 3.1259966011047364\n",
      "\n",
      "Epoch:    9/20    Loss: 3.1314466819763185\n",
      "\n",
      "Epoch:    9/20    Loss: 3.1460909247398376\n",
      "\n",
      "Epoch:    9/20    Loss: 3.155772976398468\n",
      "\n",
      "Epoch:    9/20    Loss: 3.1792237215042114\n",
      "\n",
      "Epoch:   10/20    Loss: 3.0602294290361325\n",
      "\n",
      "Epoch:   10/20    Loss: 2.976290294647217\n",
      "\n",
      "Epoch:   10/20    Loss: 2.975381202220917\n",
      "\n",
      "Epoch:   10/20    Loss: 3.0028203530311584\n",
      "\n",
      "Epoch:   10/20    Loss: 3.014415699481964\n",
      "\n",
      "Epoch:   10/20    Loss: 3.032032771587372\n",
      "\n",
      "Epoch:   10/20    Loss: 3.056017189979553\n",
      "\n",
      "Epoch:   10/20    Loss: 3.0629228591918944\n",
      "\n",
      "Epoch:   10/20    Loss: 3.084999817371368\n",
      "\n",
      "Epoch:   10/20    Loss: 3.0822523169517515\n",
      "\n",
      "Epoch:   10/20    Loss: 3.108913079738617\n",
      "\n",
      "Epoch:   10/20    Loss: 3.1224657549858095\n",
      "\n",
      "Epoch:   10/20    Loss: 3.1408987340927124\n",
      "\n",
      "Epoch:   11/20    Loss: 3.025783019371269\n",
      "\n",
      "Epoch:   11/20    Loss: 2.922884066581726\n",
      "\n",
      "Epoch:   11/20    Loss: 2.941649817466736\n",
      "\n",
      "Epoch:   11/20    Loss: 2.9585668077468874\n",
      "\n",
      "Epoch:   11/20    Loss: 2.985323825836182\n",
      "\n",
      "Epoch:   11/20    Loss: 3.02369903755188\n",
      "\n",
      "Epoch:   11/20    Loss: 3.026130693912506\n",
      "\n",
      "Epoch:   11/20    Loss: 3.0362930374145507\n",
      "\n",
      "Epoch:   11/20    Loss: 3.055119038105011\n",
      "\n",
      "Epoch:   11/20    Loss: 3.0563259673118592\n",
      "\n",
      "Epoch:   11/20    Loss: 3.069406261444092\n",
      "\n",
      "Epoch:   11/20    Loss: 3.0842775220870973\n",
      "\n",
      "Epoch:   11/20    Loss: 3.092162786483765\n",
      "\n",
      "Epoch:   12/20    Loss: 2.978840507751654\n",
      "\n",
      "Epoch:   12/20    Loss: 2.9025691046714783\n",
      "\n",
      "Epoch:   12/20    Loss: 2.917048662662506\n",
      "\n",
      "Epoch:   12/20    Loss: 2.941926472663879\n",
      "\n",
      "Epoch:   12/20    Loss: 2.947035756111145\n",
      "\n",
      "Epoch:   12/20    Loss: 2.9877185368537904\n",
      "\n",
      "Epoch:   12/20    Loss: 2.958006558418274\n",
      "\n",
      "Epoch:   12/20    Loss: 2.989699240684509\n",
      "\n",
      "Epoch:   12/20    Loss: 3.0169614877700806\n",
      "\n",
      "Epoch:   12/20    Loss: 3.02794655418396\n",
      "\n",
      "Epoch:   12/20    Loss: 3.0461911039352416\n",
      "\n",
      "Epoch:   12/20    Loss: 3.048392523288727\n",
      "\n",
      "Epoch:   12/20    Loss: 3.069216878890991\n",
      "\n",
      "Epoch:   13/20    Loss: 2.9601673552812624\n",
      "\n",
      "Epoch:   13/20    Loss: 2.883261119365692\n",
      "\n",
      "Epoch:   13/20    Loss: 2.8747992753982543\n",
      "\n",
      "Epoch:   13/20    Loss: 2.8963383483886718\n",
      "\n",
      "Epoch:   13/20    Loss: 2.916727378845215\n",
      "\n",
      "Epoch:   13/20    Loss: 2.964326087474823\n",
      "\n",
      "Epoch:   13/20    Loss: 2.949622107028961\n",
      "\n",
      "Epoch:   13/20    Loss: 2.979561342716217\n",
      "\n",
      "Epoch:   13/20    Loss: 2.9805034728050233\n",
      "\n",
      "Epoch:   13/20    Loss: 2.9905107703208924\n",
      "\n",
      "Epoch:   13/20    Loss: 3.0072732243537903\n",
      "\n",
      "Epoch:   13/20    Loss: 3.026338963031769\n",
      "\n",
      "Epoch:   13/20    Loss: 3.046930862903595\n",
      "\n",
      "Epoch:   14/20    Loss: 2.9275605363294113\n",
      "\n",
      "Epoch:   14/20    Loss: 2.8502475929260256\n",
      "\n",
      "Epoch:   14/20    Loss: 2.862771394252777\n",
      "\n",
      "Epoch:   14/20    Loss: 2.882561667919159\n",
      "\n",
      "Epoch:   14/20    Loss: 2.897243094921112\n",
      "\n",
      "Epoch:   14/20    Loss: 2.9083661351203918\n",
      "\n",
      "Epoch:   14/20    Loss: 2.930526617527008\n",
      "\n",
      "Epoch:   14/20    Loss: 2.9464077491760254\n",
      "\n",
      "Epoch:   14/20    Loss: 2.9597265791893004\n",
      "\n",
      "Epoch:   14/20    Loss: 2.978413999080658\n",
      "\n",
      "Epoch:   14/20    Loss: 3.001795436382294\n",
      "\n",
      "Epoch:   14/20    Loss: 2.9906682295799256\n",
      "\n",
      "Epoch:   14/20    Loss: 3.0018206896781923\n",
      "\n",
      "Epoch:   15/20    Loss: 2.9122402781297354\n",
      "\n",
      "Epoch:   15/20    Loss: 2.8187668228149416\n",
      "\n",
      "Epoch:   15/20    Loss: 2.842384199619293\n",
      "\n",
      "Epoch:   15/20    Loss: 2.844315474510193\n",
      "\n",
      "Epoch:   15/20    Loss: 2.8778531193733214\n",
      "\n",
      "Epoch:   15/20    Loss: 2.891734115123749\n",
      "\n",
      "Epoch:   15/20    Loss: 2.920972749233246\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   15/20    Loss: 2.9228800716400145\n",
      "\n",
      "Epoch:   15/20    Loss: 2.9264612908363343\n",
      "\n",
      "Epoch:   15/20    Loss: 2.955915428161621\n",
      "\n",
      "Epoch:   15/20    Loss: 2.961476556777954\n",
      "\n",
      "Epoch:   15/20    Loss: 2.9843465938568117\n",
      "\n",
      "Epoch:   15/20    Loss: 2.9886456422805785\n",
      "\n",
      "Epoch:   16/20    Loss: 2.8871414348606237\n",
      "\n",
      "Epoch:   16/20    Loss: 2.811547480583191\n",
      "\n",
      "Epoch:   16/20    Loss: 2.833255507469177\n",
      "\n",
      "Epoch:   16/20    Loss: 2.8428696751594544\n",
      "\n",
      "Epoch:   16/20    Loss: 2.8480278248786925\n",
      "\n",
      "Epoch:   16/20    Loss: 2.8723080806732177\n",
      "\n",
      "Epoch:   16/20    Loss: 2.877551052093506\n",
      "\n",
      "Epoch:   16/20    Loss: 2.9051781878471377\n",
      "\n",
      "Epoch:   16/20    Loss: 2.9190689840316772\n",
      "\n",
      "Epoch:   16/20    Loss: 2.921810088157654\n",
      "\n",
      "Epoch:   16/20    Loss: 2.931290591239929\n",
      "\n",
      "Epoch:   16/20    Loss: 2.9598481903076173\n",
      "\n",
      "Epoch:   16/20    Loss: 2.9709917125701906\n",
      "\n",
      "Epoch:   17/20    Loss: 2.870816743817211\n",
      "\n",
      "Epoch:   17/20    Loss: 2.788284170150757\n",
      "\n",
      "Epoch:   17/20    Loss: 2.8010914664268496\n",
      "\n",
      "Epoch:   17/20    Loss: 2.8263851799964903\n",
      "\n",
      "Epoch:   17/20    Loss: 2.835260043621063\n",
      "\n",
      "Epoch:   17/20    Loss: 2.8681849069595335\n",
      "\n",
      "Epoch:   17/20    Loss: 2.867210910320282\n",
      "\n",
      "Epoch:   17/20    Loss: 2.871544455051422\n",
      "\n",
      "Epoch:   17/20    Loss: 2.8857558274269106\n",
      "\n",
      "Epoch:   17/20    Loss: 2.9355370688438414\n",
      "\n",
      "Epoch:   17/20    Loss: 2.9240857377052305\n",
      "\n",
      "Epoch:   17/20    Loss: 2.9290720887184145\n",
      "\n",
      "Epoch:   17/20    Loss: 2.9437961869239806\n",
      "\n",
      "Epoch:   18/20    Loss: 2.845562556312104\n",
      "\n",
      "Epoch:   18/20    Loss: 2.7817687706947325\n",
      "\n",
      "Epoch:   18/20    Loss: 2.777002365589142\n",
      "\n",
      "Epoch:   18/20    Loss: 2.7919545578956604\n",
      "\n",
      "Epoch:   18/20    Loss: 2.8188124890327453\n",
      "\n",
      "Epoch:   18/20    Loss: 2.8351121778488158\n",
      "\n",
      "Epoch:   18/20    Loss: 2.847171067237854\n",
      "\n",
      "Epoch:   18/20    Loss: 2.8735776057243347\n",
      "\n",
      "Epoch:   18/20    Loss: 2.881201756954193\n",
      "\n",
      "Epoch:   18/20    Loss: 2.8888998770713807\n",
      "\n",
      "Epoch:   18/20    Loss: 2.911318841934204\n",
      "\n",
      "Epoch:   18/20    Loss: 2.9323105340003965\n",
      "\n",
      "Epoch:   18/20    Loss: 2.9213191447257993\n",
      "\n",
      "Epoch:   19/20    Loss: 2.8194911883882257\n",
      "\n",
      "Epoch:   19/20    Loss: 2.752457335472107\n",
      "\n",
      "Epoch:   19/20    Loss: 2.765777934551239\n",
      "\n",
      "Epoch:   19/20    Loss: 2.801278247833252\n",
      "\n",
      "Epoch:   19/20    Loss: 2.7892819929122923\n",
      "\n",
      "Epoch:   19/20    Loss: 2.831549980163574\n",
      "\n",
      "Epoch:   19/20    Loss: 2.8429186410903933\n",
      "\n",
      "Epoch:   19/20    Loss: 2.841424045562744\n",
      "\n",
      "Epoch:   19/20    Loss: 2.864016195774078\n",
      "\n",
      "Epoch:   19/20    Loss: 2.8821362133026125\n",
      "\n",
      "Epoch:   19/20    Loss: 2.866790973186493\n",
      "\n",
      "Epoch:   19/20    Loss: 2.895444284439087\n",
      "\n",
      "Epoch:   19/20    Loss: 2.9233640098571776\n",
      "\n",
      "Epoch:   20/20    Loss: 2.8144400728142953\n",
      "\n",
      "Epoch:   20/20    Loss: 2.733746374130249\n",
      "\n",
      "Epoch:   20/20    Loss: 2.761755811214447\n",
      "\n",
      "Epoch:   20/20    Loss: 2.772027781486511\n",
      "\n",
      "Epoch:   20/20    Loss: 2.7899671335220337\n",
      "\n",
      "Epoch:   20/20    Loss: 2.809781132221222\n",
      "\n",
      "Epoch:   20/20    Loss: 2.8227707200050354\n",
      "\n",
      "Epoch:   20/20    Loss: 2.820876500606537\n",
      "\n",
      "Epoch:   20/20    Loss: 2.844075969696045\n",
      "\n",
      "Epoch:   20/20    Loss: 2.849489085197449\n",
      "\n",
      "Epoch:   20/20    Loss: 2.8758477392196657\n",
      "\n",
      "Epoch:   20/20    Loss: 2.8794643502235413\n",
      "\n",
      "Epoch:   20/20    Loss: 2.9114823298454287\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:251: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "# create model and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "# saving the trained model\n",
    "helper.save_model('./save/trained_rnn', trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Checkpoint\n",
    "\n",
    "After running the above training cell, we will same our model by name, `trained_rnn`. We can use this checkpoint to resume our progress by running the next cell, which will load in our word:id dictionaries _and_ load in our saved model by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create checkpoint\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "trained_rnn = helper.load_model('./save/trained_rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Movie Script\n",
    "Now we will use our trained model to generate a new, \"fake\" movie script in this section.\n",
    "\n",
    "### Generate Text\n",
    "We will use the `generate` function below to generate our text. The network will start with a single word and repeat its predictions until it reaches a set length. It takes a word id to start with, `prime_id`, and generates a set length of text, `predict_len`. Also we will use topk sampling to introduce some randomness in choosing the most likely next word, given an output set of word scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    \"\"\"\n",
    "    Generate text using the neural network\n",
    "    :param decoder: The PyTorch Module that holds the trained neural network\n",
    "    :param prime_id: The word id to start the first prediction\n",
    "    :param int_to_vocab: Dict of word id keys to word values\n",
    "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
    "    :param pad_value: The value used to pad a sequence\n",
    "    :param predict_len: The length of text to generate\n",
    "    :return: The generated text\n",
    "    \"\"\"\n",
    "    rnn.eval()\n",
    "    \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next word index with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "        # retrieve that word from the dictionary\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
    "        current_seq = current_seq.cpu().numpy() #code error\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "    # return all the sentences\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a New Script\n",
    "To generate the text, we need to set `gen_length` to the length of script we want to generate and set `prime_word` to a word included in the source data (ex. \"kramer\") to start the prediction:\n",
    "\n",
    "We can set the prime word to _any word_ in our dictionary, but it's best to start with a name for generating a script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kramer: crowded liquid.\n",
      "\n",
      "jerry: what?\n",
      "\n",
      "jerry: i don't know.\n",
      "\n",
      "george:(shouting) i have to tell you that. i can't even have to do it.\n",
      "\n",
      "jerry: well, i think i'm getting a free sample.\n",
      "\n",
      "elaine:(to jerry) i told you not to tell you this thing!\n",
      "\n",
      "jerry: i don't know if i was eighteen- worthy.\n",
      "\n",
      "elaine: oh, well, that's the thing.\n",
      "\n",
      "george: what are you talking about?(he picks it up) i was in the sauna with my cousin holly.\n",
      "\n",
      "kramer: oh, no!\n",
      "\n",
      "george: hey.\n",
      "\n",
      "elaine: oh.\n",
      "\n",
      "jerry:(to kramer) so, i guess i could help you out. i'm gonna get going.\n",
      "\n",
      "jerry:(to jerry) hey, i gotta tell you, i'm sorry to disturb you.\n",
      "\n",
      "jerry:(to the intercom) hello? yeah, tommy fries is...\n",
      "\n",
      "george:(interrupting) i don't wanna see this. i don't know what to do with it.\n",
      "\n",
      "kramer: oh.\n",
      "\n",
      "george:(to george, to himself) you know, you should be ashamed of yourself!\n",
      "\n",
      "elaine: oh my god, you know, i really think you're gonna be the one who wants you to do that.\n",
      "\n",
      "jerry: i think i could.\n",
      "\n",
      "elaine:(smiling, convincing) oh, you idiots!\n",
      "\n",
      "jerry:(to the phone) hello?(listens) yeah, yeah i know. i gotta tell you.(exits)\n",
      "\n",
      "jerry: hey, i gotta skedaddle. i gotta get my mail cleaned.\n",
      "\n",
      "jerry: oh, i don't want to talk to him.\n",
      "\n",
      "kramer: hey.\n",
      "\n",
      "jerry: hey.\n",
      "\n",
      "kramer: hey, hey, you know...............\n",
      "\n",
      "jerry: oh.\n",
      "\n",
      "elaine: oh, come on, jerry, please. i don't want to sit down there, i\n"
     ]
    }
   ],
   "source": [
    "# run the cell multiple times to get different results!\n",
    "gen_length = 400 # modify the length to your preference\n",
    "prime_word = 'kramer' # name for starting the script\n",
    "\n",
    "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save scripts\n",
    "\n",
    "Once we have a script that we like (or find interesting), we can save it to a text file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save script to a text file\n",
    "f =  open(\"generated_script_1.txt\",\"w\")\n",
    "f.write(generated_script)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
